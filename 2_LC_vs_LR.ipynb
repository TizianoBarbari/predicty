{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_LC_vs_LR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_Aw0Laq0jxA"
      },
      "source": [
        "\n",
        "# Classification problem: Linear Classification (LC) vs Logistic Regression (LR)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAPg58xm2Dah"
      },
      "source": [
        "## Linear Classification Model\n",
        " The parameter vector theta includes the bias, so we append a 1 to the feature vector x.\n",
        "The prediction is $y = sign(h_{\\theta}) = sign(\\theta^Tx)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UAIeEut2Eqd"
      },
      "source": [
        "##########################################################\n",
        "# Compute the prediction of a linear classification model\n",
        "##########################################################\n",
        "\n",
        "def predict_lc(x, theta):\n",
        "    '''\n",
        "    Inputs:\n",
        "        x: np.ndarray of shape (num_samples, num_features + 1)\n",
        "        theta: np.ndarray of shape (num_features + 1, 1)\n",
        "    Outputs:\n",
        "        y: np.ndarray of shape (num_samples, 1)\n",
        "    '''\n",
        "    # y = np.sign( np.dot(  theta.T, x.T) )\n",
        "    y = np.sign( np.dot( x, theta ) )\n",
        "    return y"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzWQ4GTj6mWz"
      },
      "source": [
        "#########################################################\n",
        "# Loss function: mean squared error (in vectorised form)\n",
        "#########################################################\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    '''\n",
        "    Inputs:\n",
        "        y_true: np.ndarray of shape (m, 1)\n",
        "        y_pred: np.ndarray of shape (m, 1)\n",
        "        (m is the number of samples)\n",
        "    '''\n",
        "    m = len(y_pred)\n",
        "    # m = len(y_true)\n",
        "    cost = y_pred - y_true\n",
        "    J =  np.dot(cost.T, cost) / (2 * m)\n",
        "    return J"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWUyXT2iegzS"
      },
      "source": [
        "####################################\n",
        "# Gradient of the MSE loss function\n",
        "####################################\n",
        "\n",
        "def gradient(y_true, y_pred, x):\n",
        "    '''\n",
        "    Inputs:\n",
        "        y_true: np.ndarray of shape (m, )\n",
        "        y_pred: np.ndarray of shape (m, )\n",
        "        x: np.ndarray of shape (m, 3)\n",
        "    Outputs:\n",
        "        dJ: np.array of shape (3, 1)\n",
        "    '''\n",
        "    # Reshape arrays\n",
        "    y_true = y_true.reshape(-1, 1) # new shape (m, 1)\n",
        "    y_pred = y_pred.reshape(-1, 1) # new shape (m, 1)\n",
        "\n",
        "    m = len(y_pred) # = len(y_true), number of samples    \n",
        "    dJ = np.dot( (y_pred - y_true).T, x ).T / m\n",
        "    return dJ"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jnxI7c6przQ"
      },
      "source": [
        "#######################################################\n",
        "# Optimize Linear Classification with Gradient Descent\n",
        "#######################################################\n",
        "\n",
        "def gradient_descent(x, y, activation_function, cost_function, gradient_function,\n",
        "                     epochs = 400, seed = 1234, learning_rate = 0.01, print_every = 10):\n",
        "    '''\n",
        "    Inputs:\n",
        "        x: np.ndarray input data of shape [num_samples, num_feat + 1]\n",
        "        y: np.ndarray target data of shape [num_samples, 1]\n",
        "        ...\n",
        "        gradient_function: the gradient of the cost function\n",
        "        ...\n",
        "    Outputs:\n",
        "        theta: the optimal parameter vector\n",
        "        loss: th loss vector\n",
        "    '''\n",
        "    # Initialize theta parameters\n",
        "    np.random.seed(seed);\n",
        "    theta = np.random.normal(0, 0.001, size = (x.shape[1], 1)) / np.sqrt(2)\n",
        "\n",
        "    loss = []\n",
        "    print('Training...')\n",
        "    print(''.join(['=' for _ in range(40)]))\n",
        "    # print(\"=\" * 40)\n",
        "\n",
        "    # Iterations of gradient descent\n",
        "    for epoch in range(epochs + 1):\n",
        "        loss_epoch = [];\n",
        "\n",
        "        # Model prediction (with possible activation function)\n",
        "        z = x.dot(theta);\n",
        "        h = activation_function(z) if activation_function is not None else z;\n",
        "\n",
        "        # update loss (with the chosen cost function)\n",
        "        loss += [cost_function(y, h)];\n",
        "\n",
        "        # gradient computation and parameters update (with the chosen gradient function and learning rate)\n",
        "        dJ = gradient_function(y, h, x);\n",
        "        theta = theta - learning_rate * dJ;\n",
        "\n",
        "        # Print loss information\n",
        "        if epoch % print_every == 0:\n",
        "            print(f'Epoch {epoch}: Loss {loss[-1]}');\n",
        "\n",
        "    loss = np.array(loss).reshape(-1) # this is probably useful\n",
        "    return theta, loss"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCtDcOpGFjqY"
      },
      "source": [
        "################################################################################\n",
        "# Evaluate Linear Classification (using \"theta\", i.e. the optimised parameters,\n",
        "# and predict_lc(x, theta) = sign(theta . x)\n",
        "################################################################################\n",
        "\n",
        "def evaluate_lc(x, theta, y):\n",
        "    return predict_lc(x, theta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeBf35BEU1eu"
      },
      "source": [
        "# Logistic Regression Model\n",
        "\n",
        "Logistic regression model does not use the sign function; it uses a sigmoid function\n",
        "\n",
        "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "as an activation function:\n",
        "\n",
        "$$h_{\\theta}(x) = g(\\theta^Tx)$$.\n",
        "\n",
        "The prediction can be made considering 0.5 as a threshold:\n",
        "\n",
        "$$y = round(h_{\\theta}(x)) = round\\left(\\frac{1}{1 + e^{-\\theta^Tx}}\\right)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl14yxtKU2qj"
      },
      "source": [
        "################################\n",
        "# A common sigmoid function\n",
        "################################\n",
        "\n",
        "def sigmoid(z):\n",
        "    '''\n",
        "    Inputs:\n",
        "        z: np.ndaray of shape (m, 3)\n",
        "    Outputs:\n",
        "        s: np.ndarray of shape (m, 3) where s[i, j] = g(z[i, j])\n",
        "    '''\n",
        "    return np.divide(1, 1 + np.exp(np.negative(z)))"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5Sg_xenc43i"
      },
      "source": [
        "## Cross entropy loss function \n",
        "\n",
        "$$J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m \\left[ -y^{(i)}\\log (h_{\\theta}(x^{(i)})) - (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)})) \\right]$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZedRn55QVjP8"
      },
      "source": [
        "##############################\n",
        "# Cross entropy loss function \n",
        "##############################\n",
        "\n",
        "def xent(y_true, y_pred):\n",
        "    '''\n",
        "    Inputs:\n",
        "        y_true: np.ndarray of shape (m,)\n",
        "        y_pred: np.ndarray of shape (m,) ( y_pred == h_theta{x^i} )\n",
        "    Outputs:\n",
        "        J: float\n",
        "    '''\n",
        "    m = len(y_true) # = len(y_pred) == num_samples\n",
        "    J = np.negative( np.dot(y_true, np.log(y_pred)) + np.dot((1 - y_true), np.log(1 - y_pred)) ) / m\n",
        "    return J"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL2biOa1rsfU"
      },
      "source": [
        "################################################################################\n",
        "# Evaluate Logistic Classification (using \"theta\", i.e. the optimised parameters,\n",
        "# and predict_lr(x, theta) = round(sigmoid(theta . x))\n",
        "################################################################################\n",
        "\n",
        "def predict_lr(x, theta):\n",
        "    return np.round(sigmoid(np.dot(x, theta)))"
      ],
      "execution_count": 81,
      "outputs": []
    }
  ]
}